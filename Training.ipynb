{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12612865,"sourceType":"datasetVersion","datasetId":7967657}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T23:49:09.578194Z","iopub.execute_input":"2025-09-13T23:49:09.578582Z","iopub.status.idle":"2025-09-13T23:49:11.564453Z","shell.execute_reply.started":"2025-09-13T23:49:09.578470Z","shell.execute_reply":"2025-09-13T23:49:11.563318Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/openwebtext-2gb/openwebtext_2GB.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/Playmaker3334/gpt-oss-from-scratch.git\n%cd gpt-oss-from-scratch/gpt_oss_20b\n!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T23:49:11.565261Z","iopub.execute_input":"2025-09-13T23:49:11.565572Z","iopub.status.idle":"2025-09-13T23:50:29.179865Z","shell.execute_reply.started":"2025-09-13T23:49:11.565552Z","shell.execute_reply":"2025-09-13T23:50:29.179008Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'gpt-oss-from-scratch'...\nremote: Enumerating objects: 287, done.\u001b[K\nremote: Counting objects: 100% (287/287), done.\u001b[K\nremote: Compressing objects: 100% (187/187), done.\u001b[K\nremote: Total 287 (delta 147), reused 231 (delta 91), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (287/287), 229.14 KiB | 7.90 MiB/s, done.\nResolving deltas: 100% (147/147), done.\n/kaggle/working/gpt-oss-from-scratch/gpt_oss_20b\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\nRequirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.52.4)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.6.0)\nRequirement already satisfied: tokenizers>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.21.2)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.26.4)\nRequirement already satisfied: tqdm>=4.62.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2024.11.6)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (7.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->-r requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (0.33.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (2.32.4)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (0.70.16)\nCollecting fsspec (from torch>=1.13.0->-r requirements.txt (line 1))\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r requirements.txt (line 5)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r requirements.txt (line 5)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r requirements.txt (line 5)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r requirements.txt (line 5)) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r requirements.txt (line 5)) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->-r requirements.txt (line 5)) (2.4.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 3)) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.20.0->-r requirements.txt (line 2)) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->-r requirements.txt (line 1)) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r requirements.txt (line 5)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->-r requirements.txt (line 5)) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->-r requirements.txt (line 5)) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->-r requirements.txt (line 5)) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.0.0->-r requirements.txt (line 3)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.0.0->-r requirements.txt (line 3)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.0.0->-r requirements.txt (line 3)) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 3)) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 3)) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 3)) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 3)) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 3)) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 3)) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 3)) (1.20.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->-r requirements.txt (line 5)) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.0.0->-r requirements.txt (line 3)) (1.17.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport time\nimport subprocess\nfrom io import StringIO\n\n# Configuration\nSOURCE_FILE = \"/kaggle/input/openwebtext-2gb/openwebtext_2GB.txt\"\nINTERMEDIATE_FILE = \"/kaggle/working/small_data.txt\"\nTRAIN_OUT = \"/kaggle/working/train.txt\"\nEVAL_OUT = \"/kaggle/working/eval.txt\"\nMAX_MB = 150\nINTERMEDIATE_MB = 500\nEVAL_RATIO = 0.10\nMIN_CHARS = 100\nTARGET_CHARS = 4000\nHARD_MAX_CHARS = 6000\nSPLIT_LOOKBACK = 1200\nSEED = 42\n\ndef create_intermediate_file():\n    \"\"\"Create intermediate file using head command for faster repeated processing.\"\"\"\n    print(f\"=== Step 1: Creating intermediate file ({INTERMEDIATE_MB}MB) ===\")\n    step1_start = time.time()\n    \n    # Create intermediate file\n    cmd = f\"head -c {INTERMEDIATE_MB}M {SOURCE_FILE} > {INTERMEDIATE_FILE}\"\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    \n    if result.returncode != 0:\n        print(f\"Error creating intermediate file: {result.stderr}\")\n        return False\n    \n    step1_time = time.time() - step1_start\n    \n    # Check file size\n    file_info = subprocess.run([\"ls\", \"-lh\", INTERMEDIATE_FILE], \n                              capture_output=True, text=True)\n    print(f\"Intermediate file created: {file_info.stdout.strip()}\")\n    \n    # Count lines\n    line_count = subprocess.run([\"wc\", \"-l\", INTERMEDIATE_FILE], \n                               capture_output=True, text=True)\n    print(f\"Line count: {line_count.stdout.strip()}\")\n    \n    # Preview first few lines to understand structure\n    preview = subprocess.run([\"head\", \"-5\", INTERMEDIATE_FILE], \n                            capture_output=True, text=True)\n    print(\"Sample lines:\")\n    for i, line in enumerate(preview.stdout.strip().split('\\n')[:3]):\n        print(f\"  Line {i+1} ({len(line)} chars): {line[:100]}...\")\n    \n    print(f\"Step 1 completed in {step1_time:.2f}s\")\n    print()\n    \n    return True\n\ndef cut_chunk(text):\n    \"\"\"Optimized chunk cutting with reduced string operations.\"\"\"\n    if len(text) < HARD_MAX_CHARS:\n        return None, text\n    \n    # Find sentence boundary more efficiently\n    tail_start = max(0, len(text) - SPLIT_LOOKBACK)\n    tail = text[tail_start:]\n    \n    # Use single pass to find last sentence boundary\n    last_pos = -1\n    for i in range(len(tail) - 1, -1, -1):\n        if tail[i] in '.!?。！？':\n            last_pos = tail_start + i + 1\n            break\n    \n    cut_at = last_pos if last_pos > 0 else TARGET_CHARS\n    return text[:cut_at].strip(), text[cut_at:].lstrip()\n\ndef process_dataset():\n    \"\"\"Process the intermediate file into train and eval datasets - each line is a document.\"\"\"\n    print(f\"=== Step 2: Processing dataset (target: {MAX_MB}MB) ===\")\n    step2_start = time.time()\n    \n    random.seed(SEED)\n    os.makedirs(\"/kaggle/working\", exist_ok=True)\n    \n    max_bytes = MAX_MB * 1024 * 1024\n    total_bytes_written = 0\n    total_samples = 0\n    train_samples_count = 0\n    eval_samples_count = 0\n    train_bytes_total = 0\n    eval_bytes_total = 0\n    \n    print(f\"Processing with {EVAL_RATIO:.0%} eval ratio, min_chars={MIN_CHARS}\")\n    print(\"Processing each line as a separate document...\")\n    \n    with open(TRAIN_OUT, \"w\", encoding=\"utf-8\", buffering=8192) as f_train, \\\n         open(EVAL_OUT, \"w\", encoding=\"utf-8\", buffering=8192) as f_eval, \\\n         open(INTERMEDIATE_FILE, \"r\", encoding=\"utf-8\", buffering=8192) as f_in:\n        \n        lines_processed = 0\n        short_lines_skipped = 0\n        \n        for line in f_in:\n            lines_processed += 1\n            \n            if lines_processed % 10000 == 0:\n                mb_used = total_bytes_written / (1024 * 1024)\n                print(f\"  Lines: {lines_processed}, Samples: {total_samples}, MB: {mb_used:.1f}\")\n            \n            # Check if we've reached the limit\n            if total_bytes_written >= max_bytes:\n                print(f\"  Reached {MAX_MB}MB limit\")\n                break\n            \n            line = line.strip()\n            \n            # Skip empty lines\n            if not line:\n                continue\n                \n            # Check if line meets minimum length requirement\n            if len(line) < MIN_CHARS:\n                short_lines_skipped += 1\n                continue\n                \n            # Process this line as a document\n            current_text = line\n            \n            # Handle very long lines by chunking\n            while len(current_text) >= HARD_MAX_CHARS:\n                chunk, current_text = cut_chunk(current_text)\n                if chunk and len(chunk) >= MIN_CHARS:\n                    # Process this chunk\n                    chunk_with_newline = chunk + \"\\n\"\n                    chunk_bytes = len(chunk_with_newline.encode('utf-8'))\n                    \n                    if total_bytes_written + chunk_bytes > max_bytes:\n                        break\n                    \n                    # Assign to train or eval\n                    if random.random() < EVAL_RATIO:\n                        f_eval.write(chunk_with_newline)\n                        eval_samples_count += 1\n                        eval_bytes_total += chunk_bytes\n                    else:\n                        f_train.write(chunk_with_newline)\n                        train_samples_count += 1\n                        train_bytes_total += chunk_bytes\n                    \n                    total_bytes_written += chunk_bytes\n                    total_samples += 1\n            \n            # Process remaining text if long enough\n            if len(current_text) >= MIN_CHARS:\n                sample_with_newline = current_text + \"\\n\"\n                sample_bytes = len(sample_with_newline.encode('utf-8'))\n                \n                if total_bytes_written + sample_bytes <= max_bytes:\n                    # Assign to train or eval\n                    if random.random() < EVAL_RATIO:\n                        f_eval.write(sample_with_newline)\n                        eval_samples_count += 1\n                        eval_bytes_total += sample_bytes\n                    else:\n                        f_train.write(sample_with_newline)\n                        train_samples_count += 1\n                        train_bytes_total += sample_bytes\n                    \n                    total_bytes_written += sample_bytes\n                    total_samples += 1\n            else:\n                short_lines_skipped += 1\n    \n    step2_time = time.time() - step2_start\n    final_mb = total_bytes_written / (1024 * 1024)\n    train_mb = train_bytes_total / (1024 * 1024)\n    eval_mb = eval_bytes_total / (1024 * 1024)\n    \n    print(f\"\\nStep 2 Debug Info:\")\n    print(f\"  Lines processed: {lines_processed}\")\n    print(f\"  Short lines skipped: {short_lines_skipped}\")\n    print(f\"  Total samples created: {total_samples}\")\n    print(f\"  Average sample size: {final_mb*1024/total_samples:.1f}KB\" if total_samples > 0 else \"  No samples created\")\n    \n    print(f\"\\nStep 2 completed in {step2_time:.2f}s\")\n    if step2_time > 0 and final_mb > 0:\n        print(f\"Processing speed: {final_mb/step2_time:.1f}MB/s\")\n    \n    return {\n        'total_samples': total_samples,\n        'train_samples': train_samples_count,\n        'eval_samples': eval_samples_count,\n        'total_mb': final_mb,\n        'train_mb': train_mb,\n        'eval_mb': eval_mb,\n        'processing_time': step2_time,\n        'lines_processed': lines_processed,\n        'short_lines_skipped': short_lines_skipped\n    }\n\ndef main():\n    \"\"\"Main function that orchestrates the entire process.\"\"\"\n    print(\"=\" * 50)\n    print(\"DATASET PROCESSOR - TWO-STEP APPROACH\")\n    print(\"=\" * 50)\n    \n    total_start_time = time.time()\n    \n    # Step 1: Create intermediate file\n    if not create_intermediate_file():\n        print(\"Failed to create intermediate file. Exiting.\")\n        return\n    \n    # Step 2: Process dataset\n    results = process_dataset()\n    \n    # Final summary\n    total_time = time.time() - total_start_time\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"FINAL SUMMARY\")\n    print(\"=\" * 50)\n    print(f\"Lines processed: {results['lines_processed']:,}\")\n    print(f\"Short lines skipped: {results['short_lines_skipped']:,}\")\n    print(f\"Total samples processed: {results['total_samples']:,}\")\n    print(f\"Train samples: {results['train_samples']:,} ({results['train_mb']:.1f}MB)\")\n    print(f\"Eval samples: {results['eval_samples']:,} ({results['eval_mb']:.1f}MB)\")\n    print(f\"Total output size: {results['total_mb']:.1f}MB\")\n    print(f\"Processing time: {results['processing_time']:.2f}s\")\n    print(f\"TOTAL SCRIPT TIME: {total_time:.2f}s\")\n    if total_time > 0 and results['total_mb'] > 0:\n        print(f\"Overall speed: {results['total_mb']/total_time:.1f}MB/s\")\n    \n    # Show final file sizes\n    print(\"\\nOutput files:\")\n    subprocess.run([\"ls\", \"-lh\", TRAIN_OUT, EVAL_OUT])\n    \n    # Show sample from train file\n    print(\"\\nSample from train.txt:\")\n    try:\n        sample = subprocess.run([\"head\", \"-3\", TRAIN_OUT], \n                               capture_output=True, text=True)\n        for i, line in enumerate(sample.stdout.split('\\n')[:2]):\n            if line.strip():\n                print(f\"  Sample {i+1}: {line[:100]}...\")\n    except:\n        pass\n    \n    # Cleanup intermediate file (optional)\n    try:\n        cleanup_response = input(\"\\nDelete intermediate file? (y/N): \").strip().lower()\n        if cleanup_response == 'y':\n            os.remove(INTERMEDIATE_FILE)\n            print(f\"Intermediate file {INTERMEDIATE_FILE} deleted.\")\n        else:\n            print(f\"Intermediate file kept: {INTERMEDIATE_FILE}\")\n    except:\n        print(f\"Intermediate file kept: {INTERMEDIATE_FILE}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T23:50:29.185016Z","iopub.execute_input":"2025-09-13T23:50:29.185285Z","iopub.status.idle":"2025-09-13T23:51:00.462343Z","shell.execute_reply.started":"2025-09-13T23:50:29.185245Z","shell.execute_reply":"2025-09-13T23:51:00.461320Z"}},"outputs":[{"name":"stdout","text":"==================================================\nDATASET PROCESSOR - TWO-STEP APPROACH\n==================================================\n=== Step 1: Creating intermediate file (500MB) ===\nIntermediate file created: -rw-r--r-- 1 root root 500M Sep 13 23:50 /kaggle/working/small_data.txt\nLine count: 105298 /kaggle/working/small_data.txt\nSample lines:\n  Line 1 (5516 chars): Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched do...\n  Line 2 (10286 chars): Former secretary of state Hillary Clinton meets voters at a campaign rally in St. Louis on Saturday....\n  Line 3 (6021 chars): The opinions expressed by columnists are their own and do not represent the views of Townhall.com.  ...\nStep 1 completed in 5.46s\n\n=== Step 2: Processing dataset (target: 150MB) ===\nProcessing with 10% eval ratio, min_chars=100\nProcessing each line as a separate document...\n  Lines: 10000, Samples: 10154, MB: 46.7\n  Lines: 20000, Samples: 20359, MB: 94.5\n  Lines: 30000, Samples: 30531, MB: 142.8\n  Lines: 40000, Samples: 32123, MB: 150.0\n  Lines: 50000, Samples: 32123, MB: 150.0\n  Lines: 60000, Samples: 32123, MB: 150.0\n  Lines: 70000, Samples: 32123, MB: 150.0\n  Lines: 80000, Samples: 32123, MB: 150.0\n  Lines: 90000, Samples: 32123, MB: 150.0\n  Lines: 100000, Samples: 32123, MB: 150.0\n\nStep 2 Debug Info:\n  Lines processed: 105299\n  Short lines skipped: 21742\n  Total samples created: 32123\n  Average sample size: 4.8KB\n\nStep 2 completed in 3.63s\nProcessing speed: 41.3MB/s\n\n==================================================\nFINAL SUMMARY\n==================================================\nLines processed: 105,299\nShort lines skipped: 21,742\nTotal samples processed: 32,123\nTrain samples: 28,935 (134.8MB)\nEval samples: 3,188 (15.2MB)\nTotal output size: 150.0MB\nProcessing time: 3.63s\nTOTAL SCRIPT TIME: 9.22s\nOverall speed: 16.3MB/s\n\nOutput files:\n-rw-r--r-- 1 root root  16M Sep 13 23:50 /kaggle/working/eval.txt\n-rw-r--r-- 1 root root 135M Sep 13 23:50 /kaggle/working/train.txt\n\nSample from train.txt:\n  Sample 1: Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched do...\n  Sample 2: The opinions expressed by columnists are their own and do not represent the views of Townhall.com.  ...\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nDelete intermediate file? (y/N):  y\n"},{"name":"stdout","text":"Intermediate file /kaggle/working/small_data.txt deleted.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!python /kaggle/working/gpt-oss-from-scratch/gpt_oss_20b/main.py \\\n    --train_data /kaggle/working/train.txt \\\n    --eval_data /kaggle/working/eval.txt \\\n    --batch_size 4 \\\n    --learning_rate 3e-4 \\\n    --warmup_steps 200 \\\n    --use_epochs \\\n    --num_epochs 3 \\\n    --grad_accum_steps 16 \\\n    --log_every 50 \\\n    --save_every_epochs 1 \\\n    --mixed_precision \\\n    --gradient_checkpointing \\\n    --seq_len 256 \\\n    --generation_prompt \"The future of AI is\" \\\n    --output_dir /kaggle/working/output \\\n    --tokenizer_dir /kaggle/working/output/tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T23:51:00.463203Z","iopub.execute_input":"2025-09-13T23:51:00.463439Z","iopub.status.idle":"2025-09-14T08:52:35.682863Z","shell.execute_reply.started":"2025-09-13T23:51:00.463401Z","shell.execute_reply":"2025-09-14T08:52:35.682074Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nModel parameters: 73,380,352\n\nStarting training for 3 epochs\n\n===== Epoch 1/3 =====\n/kaggle/working/gpt-oss-from-scratch/gpt_oss_20b/main.py:312: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=self.mixed_precision):\n[Epoch 1 Step 50] loss=11.7525 ppl=127124.53 lr=0.000116 tok/s=5398\n[Epoch 1 Step 100] loss=11.7289 ppl=124198.77 lr=0.000107 tok/s=5532\n[Epoch 1 Step 150] loss=11.6637 ppl=116467.82 lr=0.000134 tok/s=5579\n[Epoch 1 Step 200] loss=11.4924 ppl=99399.75 lr=0.000206 tok/s=5602\n[Epoch 1 Step 250] loss=10.9683 ppl=64214.36 lr=0.000270 tok/s=5614\n[Epoch 1 Step 300] loss=10.2353 ppl=30785.95 lr=0.000298 tok/s=5622\n[Epoch 1 Step 350] loss=9.6609 ppl=16399.66 lr=0.000300 tok/s=5625\n[Epoch 1 Step 400] loss=9.2356 ppl=10575.96 lr=0.000300 tok/s=5626\n[Epoch 1 Step 450] loss=8.8714 ppl=7351.38 lr=0.000300 tok/s=5626\n[Epoch 1 Step 500] loss=8.5453 ppl=5286.56 lr=0.000300 tok/s=5627\n[Epoch 1 Step 550] loss=8.2983 ppl=4144.14 lr=0.000300 tok/s=5626\n[Epoch 1 Step 600] loss=8.0846 ppl=3373.55 lr=0.000300 tok/s=5626\n[Epoch 1 Step 650] loss=7.9071 ppl=2829.73 lr=0.000300 tok/s=5626\n[Epoch 1 Step 700] loss=7.7742 ppl=2468.04 lr=0.000300 tok/s=5626\n[Epoch 1 Step 750] loss=7.7317 ppl=2339.57 lr=0.000300 tok/s=5626\n[Epoch 1 Step 800] loss=7.7415 ppl=2373.21 lr=0.000300 tok/s=5626\n[Epoch 1 Step 850] loss=7.7276 ppl=2304.98 lr=0.000299 tok/s=5626\n[Epoch 1 Step 900] loss=7.7388 ppl=2332.64 lr=0.000299 tok/s=5627\n[Epoch 1 Step 950] loss=7.7671 ppl=2437.24 lr=0.000299 tok/s=5627\n[Epoch 1 Step 1000] loss=7.7511 ppl=2394.46 lr=0.000299 tok/s=5628\n[Epoch 1 Step 1050] loss=7.7166 ppl=2307.97 lr=0.000299 tok/s=5628\n[Epoch 1 Step 1100] loss=7.7040 ppl=2277.53 lr=0.000299 tok/s=5629\n[Epoch 1 Step 1150] loss=7.6748 ppl=2187.26 lr=0.000299 tok/s=5630\n[Epoch 1 Step 1200] loss=7.6663 ppl=2187.85 lr=0.000299 tok/s=5630\n[Epoch 1 Step 1250] loss=7.6871 ppl=2241.23 lr=0.000298 tok/s=5631\n[Epoch 1 Step 1300] loss=7.7118 ppl=2336.10 lr=0.000298 tok/s=5632\n[Epoch 1 Step 1350] loss=7.6689 ppl=2238.04 lr=0.000298 tok/s=5632\n[Epoch 1 Step 1400] loss=7.6546 ppl=2191.72 lr=0.000298 tok/s=5633\n[Epoch 1 Step 1450] loss=7.6780 ppl=2258.13 lr=0.000298 tok/s=5633\n[Epoch 1 Step 1500] loss=7.6424 ppl=2127.69 lr=0.000298 tok/s=5634\n[Epoch 1 Step 1550] loss=7.6312 ppl=2132.26 lr=0.000297 tok/s=5634\n[Epoch 1 Step 1600] loss=7.6267 ppl=2135.18 lr=0.000297 tok/s=5634\n[Epoch 1 Step 1650] loss=7.6085 ppl=2049.95 lr=0.000297 tok/s=5634\n[Epoch 1 Step 1700] loss=7.6083 ppl=2059.71 lr=0.000297 tok/s=5634\n[Epoch 1 Step 1750] loss=7.6207 ppl=2100.44 lr=0.000296 tok/s=5634\n[Epoch 1 Step 1800] loss=7.6150 ppl=2076.38 lr=0.000296 tok/s=5633\n[Epoch 1 Step 1850] loss=7.5789 ppl=1989.03 lr=0.000296 tok/s=5633\n[Epoch 1 Step 1900] loss=7.5684 ppl=1966.44 lr=0.000296 tok/s=5633\n[Epoch 1 Step 1950] loss=7.5655 ppl=1966.90 lr=0.000295 tok/s=5633\n[Epoch 1 Step 2000] loss=7.5899 ppl=2035.43 lr=0.000295 tok/s=5633\n[Epoch 1 Step 2050] loss=7.6219 ppl=2095.42 lr=0.000295 tok/s=5633\n[Epoch 1 Step 2100] loss=7.6160 ppl=2073.61 lr=0.000295 tok/s=5633\n[Epoch 1 Step 2150] loss=7.6034 ppl=2061.17 lr=0.000294 tok/s=5633\n[Epoch 1 Step 2200] loss=7.6212 ppl=2233.39 lr=0.000294 tok/s=5633\n[Epoch 1 Step 2250] loss=7.6267 ppl=2247.72 lr=0.000294 tok/s=5633\n[Epoch 1 Step 2300] loss=7.6017 ppl=2095.66 lr=0.000293 tok/s=5633\n[Epoch 1 Step 2350] loss=7.6031 ppl=2129.24 lr=0.000293 tok/s=5633\n[Epoch 1 Step 2400] loss=7.5622 ppl=2021.70 lr=0.000293 tok/s=5632\n[Epoch 1 Step 2450] loss=7.5300 ppl=1918.59 lr=0.000292 tok/s=5632\n[Epoch 1 Step 2500] loss=7.5319 ppl=1910.39 lr=0.000292 tok/s=5632\n[Epoch 1 Step 2550] loss=7.5001 ppl=1846.39 lr=0.000292 tok/s=5632\n[Epoch 1 Step 2600] loss=7.4922 ppl=1821.92 lr=0.000291 tok/s=5632\n[Epoch 1 Step 2650] loss=7.5118 ppl=1860.72 lr=0.000291 tok/s=5632\n[Epoch 1 Step 2700] loss=7.5118 ppl=1876.04 lr=0.000291 tok/s=5632\n[Epoch 1 Step 2750] loss=7.5220 ppl=1923.68 lr=0.000290 tok/s=5632\n[Epoch 1 Step 2800] loss=7.4890 ppl=1858.48 lr=0.000290 tok/s=5632\n[Epoch 1 Step 2850] loss=7.4574 ppl=1763.63 lr=0.000289 tok/s=5632\n[Epoch 1 Step 2900] loss=7.5355 ppl=1910.79 lr=0.000289 tok/s=5632\n[Epoch 1 Step 2950] loss=7.5670 ppl=1973.55 lr=0.000289 tok/s=5632\n[Epoch 1 Step 3000] loss=7.5468 ppl=1943.74 lr=0.000288 tok/s=5632\n[Epoch 1 Step 3050] loss=7.5742 ppl=2014.80 lr=0.000288 tok/s=5632\n[Epoch 1 Step 3100] loss=7.5818 ppl=2015.02 lr=0.000287 tok/s=5632\n[Epoch 1 Step 3150] loss=7.5845 ppl=2018.34 lr=0.000287 tok/s=5632\n[Epoch 1 Step 3200] loss=7.5568 ppl=1974.13 lr=0.000286 tok/s=5632\n[Epoch 1 Step 3250] loss=7.5016 ppl=1850.56 lr=0.000286 tok/s=5633\n[Epoch 1 Step 3300] loss=7.4788 ppl=1800.39 lr=0.000286 tok/s=5633\n[Epoch 1 Step 3350] loss=7.4916 ppl=1818.20 lr=0.000285 tok/s=5633\n[Epoch 1 Step 3400] loss=7.5725 ppl=1990.88 lr=0.000285 tok/s=5633\n[Epoch 1 Step 3450] loss=7.5850 ppl=2021.75 lr=0.000284 tok/s=5633\n[Epoch 1 Step 3500] loss=7.5581 ppl=1967.24 lr=0.000284 tok/s=5633\n[Epoch 1 Step 3550] loss=7.5682 ppl=1997.70 lr=0.000283 tok/s=5633\n[Epoch 1 Step 3600] loss=7.5500 ppl=1939.92 lr=0.000283 tok/s=5633\n[Epoch 1 Step 3650] loss=7.5540 ppl=1942.48 lr=0.000282 tok/s=5633\n[Epoch 1 Step 3700] loss=7.5884 ppl=2014.44 lr=0.000282 tok/s=5633\n[Epoch 1 Step 3750] loss=7.6368 ppl=2107.18 lr=0.000281 tok/s=5633\n[Epoch 1 Step 3800] loss=7.6056 ppl=2044.77 lr=0.000280 tok/s=5633\n[Epoch 1 Step 3850] loss=7.5696 ppl=1990.78 lr=0.000280 tok/s=5633\n[Epoch 1 Step 3900] loss=7.5842 ppl=2020.88 lr=0.000279 tok/s=5633\n[Epoch 1 Step 3950] loss=7.5704 ppl=1972.65 lr=0.000279 tok/s=5633\n[Epoch 1 Step 4000] loss=7.5725 ppl=1989.92 lr=0.000278 tok/s=5633\n[Epoch 1 Step 4050] loss=7.5842 ppl=2011.47 lr=0.000278 tok/s=5633\n[Epoch 1 Step 4100] loss=7.5641 ppl=1956.04 lr=0.000277 tok/s=5633\n[Epoch 1 Step 4150] loss=7.5758 ppl=1992.50 lr=0.000277 tok/s=5632\n[Epoch 1 Step 4200] loss=7.5973 ppl=2036.47 lr=0.000276 tok/s=5632\n[Epoch 1 Step 4250] loss=7.5874 ppl=2022.44 lr=0.000275 tok/s=5632\n[Epoch 1 Step 4300] loss=7.6232 ppl=2121.35 lr=0.000275 tok/s=5632\n[Epoch 1 Step 4350] loss=7.6360 ppl=2147.86 lr=0.000274 tok/s=5632\n[Epoch 1 Step 4400] loss=7.6316 ppl=2116.39 lr=0.000274 tok/s=5632\n[Epoch 1 Step 4450] loss=7.6326 ppl=2145.66 lr=0.000273 tok/s=5632\n[Epoch 1 Step 4500] loss=7.6108 ppl=2099.59 lr=0.000272 tok/s=5632\n[Epoch 1 Step 4550] loss=7.5590 ppl=1950.54 lr=0.000272 tok/s=5632\n[Epoch 1 Step 4600] loss=7.5382 ppl=1911.81 lr=0.000271 tok/s=5632\n[Epoch 1 Step 4650] loss=7.5970 ppl=2046.95 lr=0.000270 tok/s=5632\n[Epoch 1 Step 4700] loss=7.6039 ppl=2060.56 lr=0.000270 tok/s=5632\n[Epoch 1 Step 4750] loss=7.6042 ppl=2047.73 lr=0.000269 tok/s=5632\n[Epoch 1 Step 4800] loss=7.6385 ppl=2140.68 lr=0.000268 tok/s=5632\n[Epoch 1 Step 4850] loss=7.6082 ppl=2077.65 lr=0.000268 tok/s=5632\n[Epoch 1 Step 4900] loss=7.5914 ppl=2057.65 lr=0.000267 tok/s=5633\n[Epoch 1 Step 4950] loss=7.5587 ppl=1997.95 lr=0.000266 tok/s=5633\n[Epoch 1 Step 5000] loss=7.5342 ppl=1914.91 lr=0.000266 tok/s=5633\n[Epoch 1 Step 5050] loss=7.5670 ppl=1975.99 lr=0.000265 tok/s=5633\n[Epoch 1 Step 5100] loss=7.6137 ppl=2128.08 lr=0.000264 tok/s=5633\n[Epoch 1 Step 5150] loss=7.6107 ppl=2121.84 lr=0.000264 tok/s=5633\n[Epoch 1 Step 5200] loss=7.6061 ppl=2066.31 lr=0.000263 tok/s=5633\n[Epoch 1 Step 5250] loss=7.6470 ppl=2162.24 lr=0.000262 tok/s=5633\n[Epoch 1 Step 5300] loss=7.6470 ppl=2147.87 lr=0.000261 tok/s=5633\n[Epoch 1 Step 5350] loss=7.6514 ppl=2156.13 lr=0.000261 tok/s=5633\n[Epoch 1 Step 5400] loss=7.6323 ppl=2120.07 lr=0.000260 tok/s=5633\n[Epoch 1 Step 5450] loss=7.6238 ppl=2086.71 lr=0.000259 tok/s=5633\n[Epoch 1 Step 5500] loss=7.6618 ppl=2181.81 lr=0.000258 tok/s=5633\n[Epoch 1 Step 5550] loss=7.6635 ppl=2188.81 lr=0.000258 tok/s=5633\n[Epoch 1 Step 5600] loss=7.6415 ppl=2128.76 lr=0.000257 tok/s=5633\n[Epoch 1 Step 5650] loss=7.6435 ppl=2135.14 lr=0.000256 tok/s=5633\n[Epoch 1 Step 5700] loss=7.6191 ppl=2092.75 lr=0.000255 tok/s=5633\n[Epoch 1 Step 5750] loss=7.6007 ppl=2068.21 lr=0.000254 tok/s=5633\n[Epoch 1 Step 5800] loss=7.6135 ppl=2101.26 lr=0.000254 tok/s=5633\n[Epoch 1 Step 5850] loss=7.5902 ppl=2057.09 lr=0.000253 tok/s=5633\n[Epoch 1 Step 5900] loss=7.5779 ppl=2026.09 lr=0.000252 tok/s=5633\n[Epoch 1 Step 5950] loss=7.5514 ppl=1966.81 lr=0.000251 tok/s=5633\n[Epoch 1 Step 6000] loss=7.5345 ppl=1918.00 lr=0.000250 tok/s=5633\n[Epoch 1 Step 6050] loss=7.5599 ppl=1977.48 lr=0.000250 tok/s=5633\n[Epoch 1 Step 6100] loss=7.5806 ppl=2028.00 lr=0.000249 tok/s=5633\n[Epoch 1 Step 6150] loss=7.5828 ppl=2030.31 lr=0.000248 tok/s=5633\n[Epoch 1 Step 6200] loss=7.5565 ppl=1965.24 lr=0.000247 tok/s=5633\n[Epoch 1 Step 6250] loss=7.5646 ppl=1993.26 lr=0.000246 tok/s=5633\n[Epoch 1 Step 6300] loss=7.5678 ppl=1999.35 lr=0.000246 tok/s=5634\n[Epoch 1 Step 6350] loss=7.5862 ppl=2045.17 lr=0.000245 tok/s=5634\n[Epoch 1 Step 6400] loss=7.6014 ppl=2070.93 lr=0.000244 tok/s=5634\n[Epoch 1 Step 6450] loss=7.5804 ppl=1989.24 lr=0.000243 tok/s=5634\n[Epoch 1 Step 6500] loss=7.5905 ppl=2025.62 lr=0.000242 tok/s=5634\n[Epoch 1 Step 6550] loss=7.5740 ppl=1989.04 lr=0.000241 tok/s=5634\n[Epoch 1 Step 6600] loss=7.5479 ppl=1934.70 lr=0.000240 tok/s=5634\n[Epoch 1 Step 6650] loss=7.5273 ppl=1899.85 lr=0.000240 tok/s=5634\n[Epoch 1 Step 6700] loss=7.5347 ppl=1908.62 lr=0.000239 tok/s=5635\n[Epoch 1 Step 6750] loss=7.5387 ppl=1919.27 lr=0.000238 tok/s=5635\n[Epoch 1 Step 6800] loss=7.5386 ppl=1966.49 lr=0.000237 tok/s=5635\n[Epoch 1 Step 6850] loss=7.5670 ppl=2032.00 lr=0.000236 tok/s=5635\n[Epoch 1 Step 6900] loss=7.5850 ppl=2029.20 lr=0.000235 tok/s=5635\n[Epoch 1 Step 6950] loss=7.6000 ppl=2054.43 lr=0.000234 tok/s=5635\n[Epoch 1 Step 7000] loss=7.6012 ppl=2046.19 lr=0.000233 tok/s=5635\n[Epoch 1 Step 7050] loss=7.6030 ppl=2038.87 lr=0.000232 tok/s=5636\n[Epoch 1 Step 7100] loss=7.6306 ppl=2097.12 lr=0.000231 tok/s=5636\n[Epoch 1 Step 7150] loss=7.6771 ppl=2209.25 lr=0.000230 tok/s=5636\n[Epoch 1 Step 7200] loss=7.6645 ppl=2177.47 lr=0.000230 tok/s=5636\n\nEpoch 1 completed. Avg loss: 7.7907\nEvaluation: loss=7.6653 accuracy=0.0553\n\nGeneration sample: The future of AI isThe and the, that and to the was of the the of and the for that a the but to the, of a not from be you to to a, and with to I in the in the the and\n\nCheckpoint saved: /kaggle/working/output/checkpoint_epoch_1_step_7234.pt\n--------------------------------------------------\n\n===== Epoch 2/3 =====\n[Epoch 2 Step 7250] loss=7.6634 ppl=2182.02 lr=0.000229 tok/s=5463\n[Epoch 2 Step 7300] loss=7.6478 ppl=2149.76 lr=0.000228 tok/s=5464\n[Epoch 2 Step 7350] loss=7.6111 ppl=2046.56 lr=0.000227 tok/s=5465\n[Epoch 2 Step 7400] loss=7.6077 ppl=2044.38 lr=0.000226 tok/s=5466\n[Epoch 2 Step 7450] loss=7.6163 ppl=2091.30 lr=0.000225 tok/s=5468\n[Epoch 2 Step 7500] loss=7.6509 ppl=2180.03 lr=0.000224 tok/s=5469\n[Epoch 2 Step 7550] loss=7.6653 ppl=2220.97 lr=0.000223 tok/s=5470\n[Epoch 2 Step 7600] loss=7.6290 ppl=2139.73 lr=0.000222 tok/s=5471\n[Epoch 2 Step 7650] loss=7.6282 ppl=2121.70 lr=0.000221 tok/s=5472\n[Epoch 2 Step 7700] loss=7.6378 ppl=2129.65 lr=0.000220 tok/s=5473\n[Epoch 2 Step 7750] loss=7.6203 ppl=2082.92 lr=0.000219 tok/s=5474\n[Epoch 2 Step 7800] loss=7.6090 ppl=2064.28 lr=0.000218 tok/s=5476\n[Epoch 2 Step 7850] loss=7.5881 ppl=2010.32 lr=0.000217 tok/s=5477\n[Epoch 2 Step 7900] loss=7.6080 ppl=2056.04 lr=0.000216 tok/s=5478\n[Epoch 2 Step 7950] loss=7.6200 ppl=2099.80 lr=0.000215 tok/s=5479\n[Epoch 2 Step 8000] loss=7.6112 ppl=2083.28 lr=0.000214 tok/s=5480\n[Epoch 2 Step 8050] loss=7.5905 ppl=2019.81 lr=0.000213 tok/s=5481\n[Epoch 2 Step 8100] loss=7.6115 ppl=2082.13 lr=0.000212 tok/s=5482\n[Epoch 2 Step 8150] loss=7.6098 ppl=2082.13 lr=0.000211 tok/s=5483\n[Epoch 2 Step 8200] loss=7.5975 ppl=2028.61 lr=0.000210 tok/s=5484\n[Epoch 2 Step 8250] loss=7.5955 ppl=2035.14 lr=0.000209 tok/s=5485\n[Epoch 2 Step 8300] loss=7.5757 ppl=1995.12 lr=0.000208 tok/s=5486\n[Epoch 2 Step 8350] loss=7.6191 ppl=2085.28 lr=0.000207 tok/s=5487\n[Epoch 2 Step 8400] loss=7.6113 ppl=2063.38 lr=0.000206 tok/s=5488\n[Epoch 2 Step 8450] loss=7.5624 ppl=1960.23 lr=0.000205 tok/s=5489\n[Epoch 2 Step 8500] loss=7.6030 ppl=2055.27 lr=0.000204 tok/s=5490\n[Epoch 2 Step 8550] loss=7.6734 ppl=2212.36 lr=0.000203 tok/s=5491\n[Epoch 2 Step 8600] loss=7.6516 ppl=2173.16 lr=0.000202 tok/s=5492\n[Epoch 2 Step 8650] loss=7.6195 ppl=2102.79 lr=0.000201 tok/s=5492\n[Epoch 2 Step 8700] loss=7.6387 ppl=2153.15 lr=0.000200 tok/s=5493\n[Epoch 2 Step 8750] loss=7.6289 ppl=2149.23 lr=0.000199 tok/s=5494\n[Epoch 2 Step 8800] loss=7.6052 ppl=2083.89 lr=0.000198 tok/s=5495\n[Epoch 2 Step 8850] loss=7.5987 ppl=2052.38 lr=0.000197 tok/s=5496\n[Epoch 2 Step 8900] loss=7.5660 ppl=1976.28 lr=0.000196 tok/s=5497\n[Epoch 2 Step 8950] loss=7.6061 ppl=2150.59 lr=0.000195 tok/s=5498\n[Epoch 2 Step 9000] loss=7.6409 ppl=2241.95 lr=0.000194 tok/s=5498\n[Epoch 2 Step 9050] loss=7.5892 ppl=2039.63 lr=0.000193 tok/s=5499\n[Epoch 2 Step 9100] loss=7.5400 ppl=1920.96 lr=0.000192 tok/s=5500\n[Epoch 2 Step 9150] loss=7.5321 ppl=1897.80 lr=0.000191 tok/s=5501\n[Epoch 2 Step 9200] loss=7.5231 ppl=1878.76 lr=0.000190 tok/s=5502\n[Epoch 2 Step 9250] loss=7.5369 ppl=1904.39 lr=0.000189 tok/s=5502\n[Epoch 2 Step 9300] loss=7.5365 ppl=1906.99 lr=0.000188 tok/s=5503\n[Epoch 2 Step 9350] loss=7.5283 ppl=1897.67 lr=0.000186 tok/s=5504\n[Epoch 2 Step 9400] loss=7.5754 ppl=1992.88 lr=0.000185 tok/s=5505\n[Epoch 2 Step 9450] loss=7.5707 ppl=1975.90 lr=0.000184 tok/s=5505\n[Epoch 2 Step 9500] loss=7.5459 ppl=1922.94 lr=0.000183 tok/s=5506\n[Epoch 2 Step 9550] loss=7.5220 ppl=1877.72 lr=0.000182 tok/s=5507\n[Epoch 2 Step 9600] loss=7.5442 ppl=1931.77 lr=0.000181 tok/s=5507\n[Epoch 2 Step 9650] loss=7.5755 ppl=1996.38 lr=0.000180 tok/s=5508\n[Epoch 2 Step 9700] loss=7.5671 ppl=1978.58 lr=0.000179 tok/s=5509\n[Epoch 2 Step 9750] loss=7.5658 ppl=1979.19 lr=0.000178 tok/s=5510\n[Epoch 2 Step 9800] loss=7.5709 ppl=2008.01 lr=0.000177 tok/s=5510\n[Epoch 2 Step 9850] loss=7.5706 ppl=1998.78 lr=0.000176 tok/s=5511\n[Epoch 2 Step 9900] loss=7.5553 ppl=1944.77 lr=0.000175 tok/s=5512\n[Epoch 2 Step 9950] loss=7.5577 ppl=1945.94 lr=0.000174 tok/s=5512\n[Epoch 2 Step 10000] loss=7.5576 ppl=1946.07 lr=0.000173 tok/s=5513\n[Epoch 2 Step 10050] loss=7.5558 ppl=1948.39 lr=0.000171 tok/s=5513\n[Epoch 2 Step 10100] loss=7.5417 ppl=1917.43 lr=0.000170 tok/s=5514\n[Epoch 2 Step 10150] loss=7.5234 ppl=1879.05 lr=0.000169 tok/s=5515\n[Epoch 2 Step 10200] loss=7.5397 ppl=1909.40 lr=0.000168 tok/s=5515\n[Epoch 2 Step 10250] loss=7.5310 ppl=1890.47 lr=0.000167 tok/s=5516\n[Epoch 2 Step 10300] loss=7.5093 ppl=1847.92 lr=0.000166 tok/s=5517\n[Epoch 2 Step 10350] loss=7.5263 ppl=1879.95 lr=0.000165 tok/s=5517\n[Epoch 2 Step 10400] loss=7.5520 ppl=1932.12 lr=0.000164 tok/s=5518\n[Epoch 2 Step 10450] loss=7.6051 ppl=2072.63 lr=0.000163 tok/s=5518\n[Epoch 2 Step 10500] loss=7.6260 ppl=2120.47 lr=0.000162 tok/s=5519\n[Epoch 2 Step 10550] loss=7.6149 ppl=2107.48 lr=0.000161 tok/s=5520\n[Epoch 2 Step 10600] loss=7.6056 ppl=2084.78 lr=0.000159 tok/s=5520\n[Epoch 2 Step 10650] loss=7.5536 ppl=1943.57 lr=0.000158 tok/s=5521\n[Epoch 2 Step 10700] loss=7.5624 ppl=1958.16 lr=0.000157 tok/s=5521\n[Epoch 2 Step 10750] loss=7.5642 ppl=1959.68 lr=0.000156 tok/s=5522\n[Epoch 2 Step 10800] loss=7.5666 ppl=1972.77 lr=0.000155 tok/s=5522\n[Epoch 2 Step 10850] loss=7.6184 ppl=2093.37 lr=0.000154 tok/s=5523\n[Epoch 2 Step 10900] loss=7.6175 ppl=2110.56 lr=0.000153 tok/s=5523\n[Epoch 2 Step 10950] loss=7.6138 ppl=2096.22 lr=0.000152 tok/s=5524\n[Epoch 2 Step 11000] loss=7.6288 ppl=2101.99 lr=0.000151 tok/s=5525\n[Epoch 2 Step 11050] loss=7.6025 ppl=2041.55 lr=0.000150 tok/s=5525\n[Epoch 2 Step 11100] loss=7.6124 ppl=2071.61 lr=0.000149 tok/s=5526\n[Epoch 2 Step 11150] loss=7.6600 ppl=2291.36 lr=0.000147 tok/s=5526\n[Epoch 2 Step 11200] loss=7.6532 ppl=2282.68 lr=0.000146 tok/s=5527\n[Epoch 2 Step 11250] loss=7.6161 ppl=2079.57 lr=0.000145 tok/s=5527\n[Epoch 2 Step 11300] loss=7.6239 ppl=2092.71 lr=0.000144 tok/s=5528\n[Epoch 2 Step 11350] loss=7.6415 ppl=2132.66 lr=0.000143 tok/s=5528\n[Epoch 2 Step 11400] loss=7.6241 ppl=2082.76 lr=0.000142 tok/s=5529\n[Epoch 2 Step 11450] loss=7.6367 ppl=2126.26 lr=0.000141 tok/s=5529\n[Epoch 2 Step 11500] loss=7.6206 ppl=2094.66 lr=0.000140 tok/s=5530\n[Epoch 2 Step 11550] loss=7.6255 ppl=2096.22 lr=0.000139 tok/s=5530\n[Epoch 2 Step 11600] loss=7.6505 ppl=2147.34 lr=0.000138 tok/s=5531\n[Epoch 2 Step 11650] loss=7.6405 ppl=2121.94 lr=0.000137 tok/s=5531\n[Epoch 2 Step 11700] loss=7.6364 ppl=2124.30 lr=0.000135 tok/s=5532\n[Epoch 2 Step 11750] loss=7.6271 ppl=2100.72 lr=0.000134 tok/s=5532\n[Epoch 2 Step 11800] loss=7.6120 ppl=2055.02 lr=0.000133 tok/s=5533\n[Epoch 2 Step 11850] loss=7.6145 ppl=2088.49 lr=0.000132 tok/s=5533\n[Epoch 2 Step 11900] loss=7.6209 ppl=2101.42 lr=0.000131 tok/s=5534\n[Epoch 2 Step 11950] loss=7.6325 ppl=2124.74 lr=0.000130 tok/s=5534\n[Epoch 2 Step 12000] loss=7.6209 ppl=2102.84 lr=0.000129 tok/s=5535\n[Epoch 2 Step 12050] loss=7.5878 ppl=1999.80 lr=0.000128 tok/s=5535\n[Epoch 2 Step 12100] loss=7.5976 ppl=2018.98 lr=0.000127 tok/s=5536\n[Epoch 2 Step 12150] loss=7.6142 ppl=2065.00 lr=0.000126 tok/s=5536\n[Epoch 2 Step 12200] loss=7.6322 ppl=2126.35 lr=0.000125 tok/s=5537\n[Epoch 2 Step 12250] loss=7.6383 ppl=2129.78 lr=0.000124 tok/s=5537\n[Epoch 2 Step 12300] loss=7.6545 ppl=2151.72 lr=0.000122 tok/s=5538\n[Epoch 2 Step 12350] loss=7.6386 ppl=2126.37 lr=0.000121 tok/s=5538\n[Epoch 2 Step 12400] loss=7.6318 ppl=2105.10 lr=0.000120 tok/s=5539\n[Epoch 2 Step 12450] loss=7.6528 ppl=2164.19 lr=0.000119 tok/s=5539\n[Epoch 2 Step 12500] loss=7.6267 ppl=2108.92 lr=0.000118 tok/s=5539\n[Epoch 2 Step 12550] loss=7.6264 ppl=2109.46 lr=0.000117 tok/s=5540\n[Epoch 2 Step 12600] loss=7.6292 ppl=2121.97 lr=0.000116 tok/s=5540\n[Epoch 2 Step 12650] loss=7.6362 ppl=2138.56 lr=0.000115 tok/s=5541\n[Epoch 2 Step 12700] loss=7.6349 ppl=2175.40 lr=0.000114 tok/s=5541\n[Epoch 2 Step 12750] loss=7.6348 ppl=2168.90 lr=0.000113 tok/s=5541\n[Epoch 2 Step 12800] loss=7.6039 ppl=2062.64 lr=0.000112 tok/s=5542\n[Epoch 2 Step 12850] loss=7.5793 ppl=2004.82 lr=0.000111 tok/s=5542\n[Epoch 2 Step 12900] loss=7.6146 ppl=2085.38 lr=0.000110 tok/s=5543\n[Epoch 2 Step 12950] loss=7.6202 ppl=2099.43 lr=0.000109 tok/s=5543\n[Epoch 2 Step 13000] loss=7.6249 ppl=2093.83 lr=0.000108 tok/s=5543\n[Epoch 2 Step 13050] loss=7.6396 ppl=2129.33 lr=0.000107 tok/s=5544\n[Epoch 2 Step 13100] loss=7.6605 ppl=2185.29 lr=0.000105 tok/s=5544\n[Epoch 2 Step 13150] loss=7.6694 ppl=2197.06 lr=0.000104 tok/s=5545\n[Epoch 2 Step 13200] loss=7.6475 ppl=2138.15 lr=0.000103 tok/s=5545\n[Epoch 2 Step 13250] loss=7.6233 ppl=2076.09 lr=0.000102 tok/s=5545\n[Epoch 2 Step 13300] loss=7.6199 ppl=2071.63 lr=0.000101 tok/s=5546\n[Epoch 2 Step 13350] loss=7.5947 ppl=2022.37 lr=0.000100 tok/s=5546\n[Epoch 2 Step 13400] loss=7.6065 ppl=2051.31 lr=0.000099 tok/s=5546\n[Epoch 2 Step 13450] loss=7.6250 ppl=2101.42 lr=0.000098 tok/s=5547\n[Epoch 2 Step 13500] loss=7.6299 ppl=2119.36 lr=0.000097 tok/s=5547\n[Epoch 2 Step 13550] loss=7.6247 ppl=2096.19 lr=0.000096 tok/s=5548\n[Epoch 2 Step 13600] loss=7.6157 ppl=2081.73 lr=0.000095 tok/s=5548\n[Epoch 2 Step 13650] loss=7.6388 ppl=2211.37 lr=0.000094 tok/s=5548\n[Epoch 2 Step 13700] loss=7.6106 ppl=2137.64 lr=0.000093 tok/s=5549\n[Epoch 2 Step 13750] loss=7.5907 ppl=2013.04 lr=0.000092 tok/s=5549\n[Epoch 2 Step 13800] loss=7.6438 ppl=2173.23 lr=0.000091 tok/s=5549\n[Epoch 2 Step 13850] loss=7.6549 ppl=2209.93 lr=0.000090 tok/s=5550\n[Epoch 2 Step 13900] loss=7.6050 ppl=2057.97 lr=0.000089 tok/s=5550\n[Epoch 2 Step 13950] loss=7.5964 ppl=2032.92 lr=0.000088 tok/s=5551\n[Epoch 2 Step 14000] loss=7.6146 ppl=2078.26 lr=0.000087 tok/s=5551\n[Epoch 2 Step 14050] loss=7.6190 ppl=2092.69 lr=0.000086 tok/s=5551\n[Epoch 2 Step 14100] loss=7.5983 ppl=2035.97 lr=0.000085 tok/s=5552\n[Epoch 2 Step 14150] loss=7.5721 ppl=1968.02 lr=0.000084 tok/s=5552\n[Epoch 2 Step 14200] loss=7.5865 ppl=2005.01 lr=0.000083 tok/s=5552\n[Epoch 2 Step 14250] loss=7.6019 ppl=2049.33 lr=0.000082 tok/s=5553\n[Epoch 2 Step 14300] loss=7.6053 ppl=2055.99 lr=0.000081 tok/s=5553\n[Epoch 2 Step 14350] loss=7.6129 ppl=2073.80 lr=0.000080 tok/s=5553\n[Epoch 2 Step 14400] loss=7.6147 ppl=2084.75 lr=0.000079 tok/s=5554\n[Epoch 2 Step 14450] loss=7.5799 ppl=1999.83 lr=0.000078 tok/s=5554\n\nEpoch 2 completed. Avg loss: 7.6038\nEvaluation: loss=7.6089 accuracy=0.0414\n\nGeneration sample: The future of AI is . that an. is that in an “ the a).: and the  with is, by. ( the to that is,  and on the  with a the a the in , on\n\nCheckpoint saved: /kaggle/working/output/checkpoint_epoch_2_step_14468.pt\n--------------------------------------------------\n\n===== Epoch 3/3 =====\n[Epoch 3 Step 14500] loss=7.5503 ppl=1934.57 lr=0.000077 tok/s=5469\n[Epoch 3 Step 14550] loss=7.5648 ppl=1965.83 lr=0.000076 tok/s=5470\n[Epoch 3 Step 14600] loss=7.5943 ppl=2024.47 lr=0.000075 tok/s=5470\n[Epoch 3 Step 14650] loss=7.5986 ppl=2038.18 lr=0.000074 tok/s=5471\n[Epoch 3 Step 14700] loss=7.5795 ppl=1992.39 lr=0.000074 tok/s=5471\n[Epoch 3 Step 14750] loss=7.5674 ppl=1960.86 lr=0.000073 tok/s=5472\n[Epoch 3 Step 14800] loss=7.5636 ppl=1959.83 lr=0.000072 tok/s=5473\n[Epoch 3 Step 14850] loss=7.5581 ppl=1958.55 lr=0.000071 tok/s=5473\n[Epoch 3 Step 14900] loss=7.5423 ppl=1933.77 lr=0.000070 tok/s=5474\n[Epoch 3 Step 14950] loss=7.5360 ppl=1910.58 lr=0.000069 tok/s=5474\n[Epoch 3 Step 15000] loss=7.5567 ppl=1963.00 lr=0.000068 tok/s=5475\n[Epoch 3 Step 15050] loss=7.5810 ppl=2020.97 lr=0.000067 tok/s=5475\n[Epoch 3 Step 15100] loss=7.5616 ppl=1968.88 lr=0.000066 tok/s=5476\n[Epoch 3 Step 15150] loss=7.5718 ppl=1983.95 lr=0.000065 tok/s=5477\n[Epoch 3 Step 15200] loss=7.5729 ppl=1977.33 lr=0.000064 tok/s=5477\n[Epoch 3 Step 15250] loss=7.5344 ppl=1902.45 lr=0.000063 tok/s=5478\n[Epoch 3 Step 15300] loss=7.5422 ppl=1928.97 lr=0.000063 tok/s=5478\n[Epoch 3 Step 15350] loss=7.5575 ppl=1956.98 lr=0.000062 tok/s=5479\n[Epoch 3 Step 15400] loss=7.5391 ppl=1919.44 lr=0.000061 tok/s=5479\n[Epoch 3 Step 15450] loss=7.5579 ppl=1973.33 lr=0.000060 tok/s=5480\n[Epoch 3 Step 15500] loss=7.5951 ppl=2096.50 lr=0.000059 tok/s=5480\n[Epoch 3 Step 15550] loss=7.5506 ppl=2007.99 lr=0.000058 tok/s=5481\n[Epoch 3 Step 15600] loss=7.5261 ppl=1935.41 lr=0.000057 tok/s=5481\n[Epoch 3 Step 15650] loss=7.5434 ppl=1971.60 lr=0.000056 tok/s=5482\n[Epoch 3 Step 15700] loss=7.5285 ppl=1909.67 lr=0.000056 tok/s=5483\n[Epoch 3 Step 15750] loss=7.5090 ppl=1857.00 lr=0.000055 tok/s=5483\n[Epoch 3 Step 15800] loss=7.4928 ppl=1825.48 lr=0.000054 tok/s=5484\n[Epoch 3 Step 15850] loss=7.4812 ppl=1803.68 lr=0.000053 tok/s=5484\n[Epoch 3 Step 15900] loss=7.4965 ppl=1840.16 lr=0.000052 tok/s=5485\n[Epoch 3 Step 15950] loss=7.5501 ppl=2007.06 lr=0.000051 tok/s=5485\n[Epoch 3 Step 16000] loss=7.5527 ppl=2025.51 lr=0.000051 tok/s=5486\n[Epoch 3 Step 16050] loss=7.5247 ppl=1917.31 lr=0.000050 tok/s=5486\n[Epoch 3 Step 16100] loss=7.5092 ppl=1861.80 lr=0.000049 tok/s=5487\n[Epoch 3 Step 16150] loss=7.5297 ppl=1892.79 lr=0.000048 tok/s=5487\n[Epoch 3 Step 16200] loss=7.5645 ppl=1966.36 lr=0.000047 tok/s=5488\n[Epoch 3 Step 16250] loss=7.5429 ppl=1928.29 lr=0.000047 tok/s=5488\n[Epoch 3 Step 16300] loss=7.5421 ppl=1932.61 lr=0.000046 tok/s=5489\n[Epoch 3 Step 16350] loss=7.5243 ppl=1892.99 lr=0.000045 tok/s=5489\n[Epoch 3 Step 16400] loss=7.5341 ppl=1930.00 lr=0.000044 tok/s=5490\n[Epoch 3 Step 16450] loss=7.5611 ppl=1979.30 lr=0.000043 tok/s=5490\n[Epoch 3 Step 16500] loss=7.5240 ppl=1882.10 lr=0.000043 tok/s=5491\n[Epoch 3 Step 16550] loss=7.5195 ppl=1875.77 lr=0.000042 tok/s=5491\n[Epoch 3 Step 16600] loss=7.5422 ppl=1916.46 lr=0.000041 tok/s=5492\n[Epoch 3 Step 16650] loss=7.5287 ppl=1885.85 lr=0.000040 tok/s=5492\n[Epoch 3 Step 16700] loss=7.5279 ppl=1898.98 lr=0.000040 tok/s=5493\n[Epoch 3 Step 16750] loss=7.5577 ppl=1968.75 lr=0.000039 tok/s=5493\n[Epoch 3 Step 16800] loss=7.5391 ppl=1925.33 lr=0.000038 tok/s=5493\n[Epoch 3 Step 16850] loss=7.5241 ppl=1887.69 lr=0.000037 tok/s=5494\n[Epoch 3 Step 16900] loss=7.5209 ppl=1892.81 lr=0.000037 tok/s=5494\n[Epoch 3 Step 16950] loss=7.5213 ppl=1896.91 lr=0.000036 tok/s=5495\n[Epoch 3 Step 17000] loss=7.5322 ppl=1912.22 lr=0.000035 tok/s=5495\n[Epoch 3 Step 17050] loss=7.5332 ppl=1910.50 lr=0.000035 tok/s=5496\n[Epoch 3 Step 17100] loss=7.5524 ppl=1958.26 lr=0.000034 tok/s=5496\n[Epoch 3 Step 17150] loss=7.5431 ppl=1941.91 lr=0.000033 tok/s=5497\n[Epoch 3 Step 17200] loss=7.5279 ppl=1891.39 lr=0.000033 tok/s=5497\n[Epoch 3 Step 17250] loss=7.5146 ppl=1860.72 lr=0.000032 tok/s=5498\n[Epoch 3 Step 17300] loss=7.5035 ppl=1837.14 lr=0.000031 tok/s=5498\n[Epoch 3 Step 17350] loss=7.5264 ppl=1889.87 lr=0.000031 tok/s=5498\n[Epoch 3 Step 17400] loss=7.5153 ppl=1878.16 lr=0.000030 tok/s=5499\n[Epoch 3 Step 17450] loss=7.4958 ppl=1832.84 lr=0.000029 tok/s=5499\n[Epoch 3 Step 17500] loss=7.5239 ppl=1889.62 lr=0.000029 tok/s=5500\n[Epoch 3 Step 17550] loss=7.5163 ppl=1876.23 lr=0.000028 tok/s=5500\n[Epoch 3 Step 17600] loss=7.5144 ppl=1873.41 lr=0.000027 tok/s=5501\n[Epoch 3 Step 17650] loss=7.5355 ppl=1923.63 lr=0.000027 tok/s=5501\n[Epoch 3 Step 17700] loss=7.5395 ppl=1942.45 lr=0.000026 tok/s=5501\n[Epoch 3 Step 17750] loss=7.5259 ppl=1907.98 lr=0.000025 tok/s=5502\n[Epoch 3 Step 17800] loss=7.5307 ppl=1905.95 lr=0.000025 tok/s=5502\n[Epoch 3 Step 17850] loss=7.5421 ppl=1938.93 lr=0.000024 tok/s=5503\n[Epoch 3 Step 17900] loss=7.5015 ppl=1857.39 lr=0.000024 tok/s=5503\n[Epoch 3 Step 17950] loss=7.4958 ppl=1847.33 lr=0.000023 tok/s=5504\n[Epoch 3 Step 18000] loss=7.5066 ppl=1861.47 lr=0.000022 tok/s=5504\n[Epoch 3 Step 18050] loss=7.5223 ppl=1889.26 lr=0.000022 tok/s=5504\n[Epoch 3 Step 18100] loss=7.5370 ppl=1945.02 lr=0.000021 tok/s=5505\n[Epoch 3 Step 18150] loss=7.5212 ppl=1914.08 lr=0.000021 tok/s=5505\n[Epoch 3 Step 18200] loss=7.5009 ppl=1856.30 lr=0.000020 tok/s=5506\n[Epoch 3 Step 18250] loss=7.5105 ppl=1888.95 lr=0.000020 tok/s=5506\n[Epoch 3 Step 18300] loss=7.5463 ppl=1951.34 lr=0.000019 tok/s=5506\n[Epoch 3 Step 18350] loss=7.5099 ppl=1855.51 lr=0.000019 tok/s=5507\n[Epoch 3 Step 18400] loss=7.4899 ppl=1816.43 lr=0.000018 tok/s=5507\n[Epoch 3 Step 18450] loss=7.5170 ppl=1867.81 lr=0.000018 tok/s=5508\n[Epoch 3 Step 18500] loss=7.5270 ppl=1906.62 lr=0.000017 tok/s=5508\n[Epoch 3 Step 18550] loss=7.5437 ppl=1982.69 lr=0.000017 tok/s=5508\n[Epoch 3 Step 18600] loss=7.5392 ppl=2015.95 lr=0.000016 tok/s=5509\n[Epoch 3 Step 18650] loss=7.5123 ppl=1939.04 lr=0.000016 tok/s=5509\n[Epoch 3 Step 18700] loss=7.5118 ppl=1885.94 lr=0.000015 tok/s=5509\n[Epoch 3 Step 18750] loss=7.4988 ppl=1851.26 lr=0.000015 tok/s=5510\n[Epoch 3 Step 18800] loss=7.4892 ppl=1833.09 lr=0.000014 tok/s=5510\n[Epoch 3 Step 18850] loss=7.5368 ppl=1933.51 lr=0.000014 tok/s=5511\n[Epoch 3 Step 18900] loss=7.5232 ppl=1905.79 lr=0.000013 tok/s=5511\n[Epoch 3 Step 18950] loss=7.4911 ppl=1829.11 lr=0.000013 tok/s=5511\n[Epoch 3 Step 19000] loss=7.5354 ppl=1915.21 lr=0.000012 tok/s=5512\n[Epoch 3 Step 19050] loss=7.5502 ppl=1948.80 lr=0.000012 tok/s=5512\n[Epoch 3 Step 19100] loss=7.5091 ppl=1866.12 lr=0.000011 tok/s=5512\n[Epoch 3 Step 19150] loss=7.5292 ppl=1903.66 lr=0.000011 tok/s=5513\n[Epoch 3 Step 19200] loss=7.5361 ppl=1920.18 lr=0.000011 tok/s=5513\n[Epoch 3 Step 19250] loss=7.5090 ppl=1873.11 lr=0.000010 tok/s=5513\n[Epoch 3 Step 19300] loss=7.5195 ppl=1891.86 lr=0.000010 tok/s=5514\n[Epoch 3 Step 19350] loss=7.5225 ppl=1888.13 lr=0.000009 tok/s=5514\n[Epoch 3 Step 19400] loss=7.5147 ppl=1878.59 lr=0.000009 tok/s=5515\n[Epoch 3 Step 19450] loss=7.5216 ppl=1897.37 lr=0.000009 tok/s=5515\n[Epoch 3 Step 19500] loss=7.5059 ppl=1862.24 lr=0.000008 tok/s=5515\n[Epoch 3 Step 19550] loss=7.4996 ppl=1865.46 lr=0.000008 tok/s=5516\n[Epoch 3 Step 19600] loss=7.5266 ppl=1916.98 lr=0.000008 tok/s=5516\n[Epoch 3 Step 19650] loss=7.5284 ppl=1906.66 lr=0.000007 tok/s=5516\n[Epoch 3 Step 19700] loss=7.5425 ppl=1963.81 lr=0.000007 tok/s=5517\n[Epoch 3 Step 19750] loss=7.5163 ppl=1917.23 lr=0.000007 tok/s=5517\n[Epoch 3 Step 19800] loss=7.4795 ppl=1812.50 lr=0.000006 tok/s=5517\n[Epoch 3 Step 19850] loss=7.4918 ppl=1820.31 lr=0.000006 tok/s=5518\n[Epoch 3 Step 19900] loss=7.5109 ppl=1861.91 lr=0.000006 tok/s=5518\n[Epoch 3 Step 19950] loss=7.5225 ppl=1902.80 lr=0.000005 tok/s=5518\n[Epoch 3 Step 20000] loss=7.4978 ppl=1848.75 lr=0.000005 tok/s=5519\n[Epoch 3 Step 20050] loss=7.4854 ppl=1804.95 lr=0.000005 tok/s=5519\n[Epoch 3 Step 20100] loss=7.4971 ppl=1822.90 lr=0.000005 tok/s=5519\n[Epoch 3 Step 20150] loss=7.5097 ppl=1861.87 lr=0.000004 tok/s=5520\n[Epoch 3 Step 20200] loss=7.5093 ppl=1865.96 lr=0.000004 tok/s=5520\n[Epoch 3 Step 20250] loss=7.5102 ppl=1882.83 lr=0.000004 tok/s=5520\n[Epoch 3 Step 20300] loss=7.5213 ppl=1910.31 lr=0.000004 tok/s=5521\n[Epoch 3 Step 20350] loss=7.5117 ppl=1877.51 lr=0.000003 tok/s=5521\n[Epoch 3 Step 20400] loss=7.5066 ppl=1853.77 lr=0.000003 tok/s=5521\n[Epoch 3 Step 20450] loss=7.5119 ppl=1875.78 lr=0.000003 tok/s=5522\n[Epoch 3 Step 20500] loss=7.5144 ppl=1890.53 lr=0.000003 tok/s=5522\n[Epoch 3 Step 20550] loss=7.5042 ppl=1862.23 lr=0.000002 tok/s=5522\n[Epoch 3 Step 20600] loss=7.5002 ppl=1864.36 lr=0.000002 tok/s=5522\n[Epoch 3 Step 20650] loss=7.5277 ppl=1912.64 lr=0.000002 tok/s=5523\n[Epoch 3 Step 20700] loss=7.5270 ppl=1904.40 lr=0.000002 tok/s=5523\n[Epoch 3 Step 20750] loss=7.4887 ppl=1839.66 lr=0.000002 tok/s=5523\n[Epoch 3 Step 20800] loss=7.4909 ppl=1837.28 lr=0.000002 tok/s=5524\n[Epoch 3 Step 20850] loss=7.5213 ppl=1887.61 lr=0.000001 tok/s=5524\n[Epoch 3 Step 20900] loss=7.5590 ppl=2018.09 lr=0.000001 tok/s=5524\n[Epoch 3 Step 20950] loss=7.5777 ppl=2056.26 lr=0.000001 tok/s=5525\n[Epoch 3 Step 21000] loss=7.5134 ppl=1874.08 lr=0.000001 tok/s=5525\n[Epoch 3 Step 21050] loss=7.4912 ppl=1819.70 lr=0.000001 tok/s=5525\n[Epoch 3 Step 21100] loss=7.5147 ppl=1880.59 lr=0.000001 tok/s=5525\n[Epoch 3 Step 21150] loss=7.5187 ppl=1899.59 lr=0.000001 tok/s=5526\n[Epoch 3 Step 21200] loss=7.5058 ppl=1863.96 lr=0.000001 tok/s=5526\n[Epoch 3 Step 21250] loss=7.4931 ppl=1842.94 lr=0.000000 tok/s=5526\n[Epoch 3 Step 21300] loss=7.4997 ppl=1857.17 lr=0.000000 tok/s=5527\n[Epoch 3 Step 21350] loss=7.5189 ppl=1896.73 lr=0.000000 tok/s=5527\n[Epoch 3 Step 21400] loss=7.5416 ppl=1952.81 lr=0.000000 tok/s=5527\n[Epoch 3 Step 21450] loss=7.5223 ppl=1903.46 lr=0.000000 tok/s=5528\n[Epoch 3 Step 21500] loss=7.4859 ppl=1813.93 lr=0.000000 tok/s=5528\n[Epoch 3 Step 21550] loss=7.4899 ppl=1826.98 lr=0.000000 tok/s=5528\n[Epoch 3 Step 21600] loss=7.5059 ppl=1857.67 lr=0.000000 tok/s=5528\n[Epoch 3 Step 21650] loss=7.4918 ppl=1818.50 lr=0.000000 tok/s=5529\n[Epoch 3 Step 21700] loss=7.5081 ppl=1858.92 lr=0.000000 tok/s=5529\n\nEpoch 3 completed. Avg loss: 7.5256\nEvaluation: loss=7.5418 accuracy=0.0396\n\nGeneration sample: The future of AI is with they of the the an in this the the it to. of be , a in is the the the he he and the and  the the and a been his the and been and the have the it\n\nCheckpoint saved: /kaggle/working/output/checkpoint_epoch_3_step_21702.pt\n--------------------------------------------------\n\nTraining completed!\nFinal checkpoint saved: /kaggle/working/output/checkpoint_epoch_3_step_21702.pt\nDone!\n","output_type":"stream"}],"execution_count":4}]}